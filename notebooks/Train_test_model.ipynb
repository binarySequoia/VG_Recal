{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN MODELS\n",
    "\n",
    "This notebook we train a model for each base pair length in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/home/jchan67/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import brier_score_loss, accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def decodePhed(x):\n",
    "    return 1.0-10**(-x/10.0)\n",
    "\n",
    "def encodePhed(x):\n",
    "    return -10 * np.log10(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_placeholder = \"../data/train_gamcompare/json/compared_mapped{}_sim{}.json\"\n",
    "test_file_placeholder = \"../data/test_gamcompare/json/tcompared_tmapped{}_tsim{}.json\"\n",
    "model_file_placeholder = \"../data/models/model_len{}.h5\"\n",
    "tsv_file_placeholder = \"../data/stats/test/tcompared_tmapped{}_tsim{}.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD JSON FILE AND CONVERT TO CSV FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json2csv(file):\n",
    "    df_dict = {\n",
    "        'correct': list(),\n",
    "        'mq': list(),\n",
    "        'score': list(),\n",
    "        'secondary_score' : list(),\n",
    "        'secondary_score_size':list(),\n",
    "        'identity': list(),\n",
    "        'aligner' : list(),\n",
    "        'read': list()\n",
    "    }\n",
    "    with open(file, \"r+\") as f:\n",
    "        line = f.readline()\n",
    "        #line_dict = json.loads(line) \n",
    "        i = 0\n",
    "        while(line != \"\"):\n",
    "            line_dict = json.loads(line)\n",
    "            if 'correctly_mapped' in line_dict:\n",
    "                df_dict['correct'].append(1)\n",
    "            else:\n",
    "                df_dict['correct'].append(0)\n",
    "                \n",
    "            if 'mapping_quality' in line_dict:\n",
    "                df_dict['mq'].append(line_dict['mapping_quality'])\n",
    "            else:\n",
    "                df_dict['mq'].append(0)\n",
    "            \n",
    "            if 'score' in line_dict:\n",
    "                df_dict['score'].append(line_dict['score'])\n",
    "            else:\n",
    "                df_dict['score'].append(0)\n",
    "                \n",
    "            if 'identity' in line_dict:\n",
    "                df_dict['identity'].append(line_dict['identity'])\n",
    "            else:\n",
    "                df_dict['identity'].append(0)\n",
    "                \n",
    "            if 'secondary_score' in line_dict:\n",
    "                df_dict['secondary_score'].append(line_dict['secondary_score'][0])\n",
    "                df_dict['secondary_score_size'].append(len(line_dict['secondary_score']))\n",
    "            else:\n",
    "                df_dict['secondary_score'].append(0)\n",
    "                df_dict['secondary_score_size'].append(0)\n",
    "            \n",
    "            df_dict['aligner'].append('orig')\n",
    "            df_dict['read'].append(line_dict['name'])\n",
    "            \n",
    "            line = f.readline()\n",
    "            i += 1\n",
    "        print(i)\n",
    "\n",
    "    return pd.DataFrame(df_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(train_file):  \n",
    "    df = json2csv(train_file)\n",
    "    ndf = df.copy()\n",
    "\n",
    "    ndf['mq'] = df.mq/60.0\n",
    "    ndf['score'] = df.score/df.score.max()\n",
    "    ndf['secondary_score'] = df.secondary_score/df.secondary_score.max()\n",
    "    ndf['secondary_score_size'] = df.secondary_score_size/df.secondary_score_size.max()\n",
    "\n",
    "    incorrect_amount = ndf[ndf.correct == 0]['correct'].count()\n",
    "    incorrect_amount\n",
    "\n",
    "    train_incorrect_amount = int(incorrect_amount * 0.8)\n",
    "    test_incorrect_amount = int(incorrect_amount * 0.2)\n",
    "    train_incorrect_amount,test_incorrect_amount\n",
    "\n",
    "    X = ndf.iloc[:, 1:6]\n",
    "    y = ndf.iloc[:, :1]\n",
    "\n",
    "    permu_index = np.random.permutation(X.shape[0])\n",
    "\n",
    "    X = X.iloc[permu_index, :]\n",
    "    y = y.iloc[permu_index]\n",
    "\n",
    "    X_train_correct = X[y.correct == 1].iloc[:train_incorrect_amount]\n",
    "    y_train_correct = y[y.correct == 1].iloc[:train_incorrect_amount]\n",
    "\n",
    "    X_train_incorrect = X[y.correct == 0].iloc[:train_incorrect_amount]\n",
    "    y_train_incorrect = y[y.correct == 0].iloc[:train_incorrect_amount]\n",
    "\n",
    "    X_test_correct = X[y.correct == 1].iloc[train_incorrect_amount:train_incorrect_amount+test_incorrect_amount]\n",
    "    y_test_correct = y[y.correct == 1].iloc[train_incorrect_amount:train_incorrect_amount+test_incorrect_amount]\n",
    "\n",
    "    X_test_incorrect = X[y.correct == 0].iloc[train_incorrect_amount:train_incorrect_amount+test_incorrect_amount]\n",
    "    y_test_incorrect = y[y.correct == 0].iloc[train_incorrect_amount:train_incorrect_amount+test_incorrect_amount]\n",
    "\n",
    "\n",
    "    X_train = np.append(X_train_correct, X_train_incorrect, axis=0)\n",
    "    X_test = np.append(X_test_correct, X_test_incorrect, axis=0)\n",
    "\n",
    "    y_train = np.append(y_train_correct, y_train_incorrect)\n",
    "    y_test = np.append(y_test_correct, y_test_incorrect)\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "\n",
    "    y_train_class = np.zeros((2*train_incorrect_amount, 2))\n",
    "    y_train_class[(y_train==1), : ] = [1, 0]\n",
    "    y_train_class[(y_train==0), :] = [0, 1]\n",
    "\n",
    "    y_test_class = np.zeros((2 * test_incorrect_amount, 2))\n",
    "\n",
    "    y_test_class[(y_test == 1), : ] = [1, 0]\n",
    "    y_test_class[(y_test == 0), :] = [0, 1]\n",
    "\n",
    "    return X_train, y_train_class, X_test, y_test_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD TRAINING DATA FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(test_file):\n",
    "    \n",
    "    df = json2csv(train_file)\n",
    "    ndf = df.copy()\n",
    "    ndf['mq'] = df.mq/60.0\n",
    "    ndf['score'] = df.score/df.score.max()\n",
    "    ndf['secondary_score'] = df.secondary_score/df.secondary_score.max()\n",
    "    ndf['secondary_score_size'] = df.secondary_score_size/df.secondary_score_size.max()\n",
    "    \n",
    "    X = ndf.iloc[:, 1:6]\n",
    "    labels = ndf.correct.values\n",
    "    orig = decodePhed(df.mq.values)\n",
    "    tsv_data = df.copy()\n",
    "    tsv_data = tsv_data.drop(['score', 'secondary_score', 'identity', 'secondary_score_size'], axis=1)\n",
    "    return X, labels, orig, tsv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    input_layer = 5\n",
    "    output_layer = 2 \n",
    "\n",
    "    h_layer1 = 8\n",
    "    dropout1 = 0.25\n",
    "\n",
    "    h_layer2 = 16\n",
    "    dropout2 = 0.5\n",
    "\n",
    "    h_layer3 = 16\n",
    "    dropout3 = 0.5\n",
    "\n",
    "    h_layer4 = 8\n",
    "    dropout4 = 0.5\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(h_layer1, activation='relu', input_shape=(input_layer, )))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout1))\n",
    "\n",
    "    model.add(Dense(h_layer2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout2))\n",
    "\n",
    "    model.add(Dense(h_layer3, activation='relu'))\n",
    "    model.add(Dropout(dropout3))\n",
    "\n",
    "    model.add(Dense(h_layer4, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout4))\n",
    "\n",
    "    model.add(Dense(output_layer, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING AND TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Train Data...\n"
     ]
    }
   ],
   "source": [
    "df_dict = {\n",
    "    'len' : list(),\n",
    "    'orig_brier_score' : list(),\n",
    "    'nn_brier_score' : list(),\n",
    "    'nn_accuracy': list(),\n",
    "    'orig_accuracy':list(),\n",
    "    'nn_pos_accuracy':list(),\n",
    "    'orig_pos_accuracy' : list(),\n",
    "    'nn_neg_accuracy':list(),\n",
    "    'orig_neg_accuracy' : list(),\n",
    "    'train_file' : list(),\n",
    "    'test_file': list(),\n",
    "    'model_file': list()\n",
    "}\n",
    "\n",
    "for i in range(100, 110, 10):\n",
    "    train_file = train_file_placeholder.format(i, i)\n",
    "    test_file = test_file_placeholder.format(i, i)\n",
    "    model_file = model_file_placeholder.format(i)\n",
    "    tsv_file = tsv_file_placeholder.format(i, i)\n",
    "    print(\"Loading Train Data...\")\n",
    "    X_train, y_train, X_test, y_test = get_train_data(train_file)\n",
    "    print(\"Loading Test Data...\")\n",
    "    X, labels, orig, orig_tsv_data = get_test_data(test_file)\n",
    "    recal_tsv_data = orig_tsv_data.copy()\n",
    "\n",
    "    model = get_model()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, batch_size=128, epochs=30, \n",
    "              verbose=1, validation_data=(X_test, y_test), shuffle=True)\n",
    "\n",
    "    model.save(model_file)\n",
    "\n",
    "    print(\"Predicting...\")\n",
    "    y_pred = model.predict(X)\n",
    "    nn_pred = y_pred[:, 0]\n",
    "    \n",
    "    recal_tsv_data.loc[:,'mq'] = encodePhed(y_pred[:, 1])\n",
    "    recal_tsv_data.loc[:,'aligner'] = 'recal'\n",
    "    orig_tsv_data = orig_tsv_data.append(recal_tsv_data , ignore_index=True)\n",
    "    orig_tsv_data.to_csv(tsv_file)\n",
    "    \n",
    "    \n",
    "    nn_brier_score = brier_score_loss(labels, nn_pred)\n",
    "    orig_brier_score = brier_score_loss(labels, orig)\n",
    "\n",
    "    nn_pred_class = nn_pred.copy()\n",
    "    nn_pred_class[nn_pred >= 0.5] = 1\n",
    "    nn_pred_class[nn_pred < 0.5] = 0\n",
    "\n",
    "    nn_acc = accuracy_score(labels, nn_pred_class)\n",
    "\n",
    "    orig_class = orig.copy()\n",
    "    orig_class[orig >= 0.5] = 1\n",
    "    orig_class[orig < 0.5] = 0\n",
    "\n",
    "    orig_acc = accuracy_score(labels, orig_class)\n",
    "\n",
    "    nn_pos_acc = accuracy_score(labels[labels == 1], nn_pred_class[labels == 1])\n",
    "    orig_pos_acc = accuracy_score(labels[labels == 1], orig_class[labels == 1])\n",
    "\n",
    "    nn_neg_acc = accuracy_score(labels[labels == 0], nn_pred_class[labels == 0])\n",
    "    orig_neg_acc = accuracy_score(labels[labels == 0], orig_class[labels == 0])\n",
    "    \n",
    "    df_dict['len'].append(i)\n",
    "    df_dict['orig_brier_score'].append(orig_brier_score)\n",
    "    df_dict['nn_brier_score'].append(nn_brier_score)\n",
    "    df_dict['nn_accuracy'].append(nn_acc)\n",
    "    df_dict['orig_accuracy'].append(orig_acc)\n",
    "    df_dict['nn_pos_accuracy'].append( nn_pos_acc)\n",
    "    df_dict['orig_pos_accuracy'].append(orig_pos_acc)\n",
    "    df_dict['nn_neg_accuracy'].append(nn_neg_acc)\n",
    "    df_dict['orig_neg_accuracy'].append(orig_neg_acc)\n",
    "    df_dict['train_file'].append(train_file)\n",
    "    df_dict['test_file'].append(test_file)\n",
    "    df_dict['model_file'].append(model_file)\n",
    "    \n",
    "    \n",
    "\n",
    "model_stats = pd.DataFrame(df_dict)\n",
    "model_stats.to_csv('model_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
